{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Input Instructions\n","* You need to first train and save the model:-\n","  1. Download book.csv.\n","  2. Change file_path variable to path of book.csv.\n","  3. Also change the model_path accordingly to save the model\n","* Now change the model_saved_path to the directory where the model is saved.\n","* other than this specify a resume_path which has the path to the resume to highlight\n","* And output_pdf_path as the output path of the pdf, also specify the name of the pdf to be made.\n","* Example_output pdf is the pdf we ran on and outputted our result.\n","\n","Note :- You can use find these variables using Ctrl +F in Vs Code or in most code editors.\n"," "]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-03-27T04:02:52.147023Z","iopub.status.busy":"2024-03-27T04:02:52.146604Z","iopub.status.idle":"2024-03-27T04:03:09.819322Z","shell.execute_reply":"2024-03-27T04:03:09.817872Z","shell.execute_reply.started":"2024-03-27T04:02:52.146993Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid, fd = os.forkpty()\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Collecting pdfplumber\n","  Downloading pdfplumber-0.11.0-py3-none-any.whl.metadata (39 kB)\n","Collecting pdfminer.six==20231228 (from pdfplumber)\n","  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n","Requirement already satisfied: Pillow>=9.1 in /opt/conda/lib/python3.10/site-packages (from pdfplumber) (9.5.0)\n","Collecting pypdfium2>=4.18.0 (from pdfplumber)\n","  Downloading pypdfium2-4.28.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n","Requirement already satisfied: cryptography>=36.0.0 in /opt/conda/lib/python3.10/site-packages (from pdfminer.six==20231228->pdfplumber) (41.0.7)\n","Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.16.0)\n","Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.21)\n","Downloading pdfplumber-0.11.0-py3-none-any.whl (56 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading pypdfium2-4.28.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n","Successfully installed pdfminer.six-20231228 pdfplumber-0.11.0 pypdfium2-4.28.0\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install pdfplumber\n"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-03-27T04:05:46.308220Z","iopub.status.busy":"2024-03-27T04:05:46.307680Z","iopub.status.idle":"2024-03-27T04:06:04.898160Z","shell.execute_reply":"2024-03-27T04:06:04.896467Z","shell.execute_reply.started":"2024-03-27T04:05:46.308169Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Collecting PyMuPdf\n","  Downloading PyMuPDF-1.24.0-cp310-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n","Collecting PyMuPDFb==1.24.0 (from PyMuPdf)\n","  Downloading PyMuPDFb-1.24.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\n","Downloading PyMuPDF-1.24.0-cp310-none-manylinux2014_x86_64.whl (3.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading PyMuPDFb-1.24.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.8/30.8 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n","\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPdf\n","Successfully installed PyMuPDFb-1.24.0 PyMuPdf-1.24.0\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install PyMuPdf\n"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-03-27T03:14:53.475328Z","iopub.status.busy":"2024-03-27T03:14:53.474876Z","iopub.status.idle":"2024-03-27T03:20:15.689227Z","shell.execute_reply":"2024-03-27T03:20:15.687779Z","shell.execute_reply.started":"2024-03-27T03:14:53.475291Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2a181adb54764f4ca5172df8af2ea7b5","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ce48c728ffeb4f7ba7064c2dc1caebe1","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7dfb49106f9d49e99eee0c8e1e082ba6","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"14ad4f0f2b614a38b950fda1c07a6aa8","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"57b29dc31da44a6a905140e01717cd2c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b97ab33b2a894d389e577fcf3043b7de","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4af0c4ee80cf4de7834262385b43b3ee","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n","  warnings.warn(\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["wandb version 0.16.5 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.4"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240327_031520-a8r07hil</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/fireborn/huggingface/runs/a8r07hil' target=\"_blank\">pleasant-meadow-3</a></strong> to <a href='https://wandb.ai/fireborn/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/fireborn/huggingface' target=\"_blank\">https://wandb.ai/fireborn/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/fireborn/huggingface/runs/a8r07hil' target=\"_blank\">https://wandb.ai/fireborn/huggingface/runs/a8r07hil</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [9/9 03:40, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["import pandas as pd\n","import numpy as np\n","from datasets import Dataset\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n","from sklearn.model_selection import train_test_split\n","\n","# Load the DataFrame\n","file_path = '/kaggle/input/resumes/book.csv'\n","df = pd.read_csv(file_path)  # Adjust separator based on your CSV\n","\n","# Define queries and keywords for hypothetical label assignment\n","queries = ['Frontend', 'Backend', 'Mathematics', 'Programming', 'Leadership', 'Accounting', 'AI', 'ML']\n","category_keywords = {\n","    'Frontend': ['html', 'css', 'javascript', 'react', 'vue', 'angular'],\n","    'Backend': ['node.js', 'django', 'flask', 'ruby on rails', 'express'],\n","    'Mathematics': ['algebra', 'calculus', 'statistics', 'geometry', 'probability'],\n","    'Programming': ['python', 'java', 'c++', 'go', 'rust', 'kotlin'],\n","    'Leadership': ['management', 'team lead', 'director', 'head', 'supervisor'],\n","    'Accounting': ['taxes', 'accounts receivable', 'ledger', 'financial statements', 'bookkeeping'],\n","    'AI': ['artificial intelligence', 'neural networks', 'deep learning', 'nlp', 'computer vision'],\n","    'ML': ['machine learning', 'svm', 'random forest', 'k-means', 'regression']\n","}\n","\n","def assign_query_and_label(text):\n","    query = np.random.choice(queries)\n","    keywords = category_keywords[query]\n","    label = 1 if any(keyword in text.lower() for keyword in keywords) else 0\n","    return query, label\n","\n","# Apply function to DataFrame\n","df['Query'], df['Label'] = zip(*df['Resume_str'].apply(assign_query_and_label))\n","\n","# Split into train and eval DataFrames\n","train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42)\n","\n","# Tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n","\n","def preprocess_function(examples):\n","    # Tokenize the inputs (text) and include the labels\n","    tokenized_inputs = tokenizer(examples['Resume_str'], truncation=True, padding='max_length', max_length=512)\n","    tokenized_inputs[\"labels\"] = examples[\"Label\"]  # Ensure 'Label' column exists and is correctly named\n","    return tokenized_inputs\n","\n","# Convert DataFrames to Hugging Face Datasets\n","train_dataset = Dataset.from_pandas(train_df)\n","eval_dataset = Dataset.from_pandas(eval_df)\n","\n","# Apply the tokenization preprocessing, ensuring labels are included\n","train_dataset = train_dataset.map(preprocess_function, batched=True)\n","eval_dataset = eval_dataset.map(preprocess_function, batched=True)\n","\n","# Tokenizer and Model\n","model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n","\n","# Training arguments\n","training_args = TrainingArguments(\n","    output_dir='./models/',\n","    num_train_epochs=1,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    logging_dir='./logs/',\n",")\n","\n","# Initialize Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n",")\n","\n","# Start training\n","trainer.train()\n","\n","model_path = '/kaggle/working/models'\n","model.save_pretrained(model_path)\n"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-03-27T04:12:08.061389Z","iopub.status.busy":"2024-03-27T04:12:08.060978Z","iopub.status.idle":"2024-03-27T04:12:14.439706Z","shell.execute_reply":"2024-03-27T04:12:14.438404Z","shell.execute_reply.started":"2024-03-27T04:12:08.061357Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Keywords related to query 'leadership' in the resume: ['defense', 'software', 'leadership']\n"]}],"source":["import torch\n","import pdfplumber\n","import re\n","import fitz\n","from transformers import BertTokenizer, BertModel\n","\n","# Load your previously trained tokenizer and model\n","model_saved_path = BertModel.from_pretrained('/kaggle/working/models')\n","\n","def extract_text_from_pdf(pdf_path):\n","    text = \"\"\n","    with pdfplumber.open(pdf_path) as pdf:\n","        for page in pdf.pages:\n","            text += page.extract_text()\n","    return text\n","\n","def find_keywords_in_resume(resume_text, query):\n","    # Tokenize input text and query\n","    inputs = tokenizer(query, resume_text, return_tensors='pt', padding=True, truncation=True)\n","\n","    # Get BERT embeddings\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","        query_embedding = outputs.last_hidden_state[:, 0, :]  # Query embedding\n","        resume_embeddings = outputs.last_hidden_state[:, 1:, :]  # Resume embeddings\n","\n","    # Compute cosine similarity between query and each token in resume text\n","    similarities = torch.nn.functional.cosine_similarity(query_embedding, resume_embeddings, dim=-1)\n","\n","    # Sort tokens by similarity and extract keywords\n","    sorted_indices = torch.argsort(similarities, descending=True, dim=-1)\n","    keywords = [tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][i].item()) for i in sorted_indices[0][:10]]  # Extract top 10 keywords\n","\n","    # Filter out keywords containing non-alphanumeric characters\n","    keywords = [word for word in keywords if re.match(\"^[a-zA-Z]+$\", word)]\n","    return keywords\n","\n","def highlight_words_in_pdf(pdf_path, words_to_highlight, output_pdf_path):\n","    # Open the PDF\n","    doc = fitz.open(pdf_path)\n","    \n","    for page in doc:\n","        # Search for each word on the page and highlight it\n","        for word in words_to_highlight:\n","            text_instances = page.search_for(word)\n","            \n","            # Highlight each instance found\n","            for inst in text_instances:\n","                highlight = page.add_highlight_annot(inst)\n","                highlight.update()\n","    \n","    # Save the highlighted PDF\n","    doc.save(output_pdf_path)\n","\n","# Example usage\n","resume_path = \"/kaggle/input/resume-dataset/data/data/ACCOUNTANT/10554236.pdf\"\n","query = \"leadership\"\n","resume_text = extract_text_from_pdf(resume_path)\n","keywords = find_keywords_in_resume(resume_text, query)\n","print(\"Keywords related to query '{}' in the resume: {}\".format(query, keywords))\n","\n","output_pdf_path = \"/kaggle/working/new_file.pdf\"\n","highlight_words_in_pdf(resume_path, keywords, output_pdf_path)\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":1519260,"sourceId":2508632,"sourceType":"datasetVersion"},{"datasetId":4673917,"sourceId":7948221,"sourceType":"datasetVersion"}],"dockerImageVersionId":30673,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
